Greg Meredith and I have a short paper that's been accepted to HDRA 2015 on modeling pi calculus with 2-categories.  To explain what we're doing, I'd like to start with lambda calculus first.

Alonzo Church invented the lambda calculus in 1934 as part of his attack on Hilbert's tenth problem.  It was arguably the first "high-level" programming language.  Turing invented his eponymous machines two years later, also to solve Hilbert's tenth problem, only to discover that Church had beaten him to it; in 1937 he proved that the two approches were equivalent in power.  Since Turing machines were much more "mechanical" than the lambda calculus, the development of computing machines relied far more on Turing's approach, and it was only decades later that people started writing compilers for more friendly programming languages.  It has been quipped that "the history of programming languages is the piecemeal rediscovery of the lambda calculus by computer scientists."

The lambda calculus consists of a set of terms together with a reduction relation on the terms.  The set \(T\) of terms is defined recursively, parametric in a set \(V\) of variables.  The base terms are the variables: if \(x\) is an element of \(V\), then \(Var x\) is a term in \(T\).  Next, given any two terms \(t, t' \in T\), we can apply one to the other to get \(App t, t'\).  Finally, we can abstract out a variable from a term: given a variable \(x\) and a term \(t,\) we get a term \(Lam x, t\).  Church denoted this abstraction operation with the Greek letter lambda, giving the calculus its name.  

The term constructors define an algebra, a functor \(LC\) from Set to Set that takes any set of variables \(V\) to the set of terms \(T = LC(V)\).  The term constructors themselves become functions:
\[
\begin{array}{rl}
Var\maps & V \to T\\
App\maps & T \times T \to T\\
Lam\maps & V \times T \to T
\end{array}
\]

Church described three relations on terms.  The first relation, alpha, relates any two lambda abstractions that differ only in the variable name.  This is exactly the same as when we consider the function \(f(x) = x^2\) to be identical to the function \(f(y) = y^2\).  The third relation, eta, says that there's no difference between a function \(f\) and a "middle-man" function that gets an input \(x\) and applies the function \(f\) to it: \(Lam x App f x = f\).  Both alpha and eta are equivalences.

The really important relation is the second one, "beta reduction".  It says that when we have a lambda abstraction \(Lam x, t\) applied to a term \(t'\), then we replace every free occurrence of \(x\) in \(t\) by \(t'\):
\[ App Lam x t t' \Downarrow_\beta t{t' / x},\]
where we read the right hand side as "\(t\) with \(t'\) replacing \(x\)".  We see a similar replacement in action when we compose functions:
\[
\begin{array}{rl}
f(x) = & x + 1 \\
g(y) = & y^2 \\
g(f(x)) = (x + 1)^2 \\
\end{array}
\]
We say a term has a normal form if there's some sequence of beta reductions that leads to a term where no beta reduction applies.  When the beta rule applies in more than one place in a term, it doesn't matter which one you choose to do first: any sequence of betas that leads to a normal form will lead to the same normal form.  This property of beta reduction is called "confluence".

Church introduced the notion of types to avoid terms with no normal form---or in programming language parlance, programs that never stop.  Modern programming languages use types to avoid programmer mistakes and encode properties about the program.  We can get a set of well-typed terms by using a coalgebra instead of an algebra.  If we start with a term of type \(A\), we can either generate a variable of type \(A\), an application of a term of type \(B \multimap A\) to a term of type \(B\), or the abstraction of a variable of type \(B\) from a term of type \(A\).  The term destructors become functions between sets annotated by types:
\[
\begin{array}{rl}
Var\maps & T^A \to V^A\\
App\maps & T^A \to T^{B \to A} \times T^B\\
Lam\maps & T^A \to V^B \times T^A
\end{array}
\]

Suppose we want the most general type for the term \(Lam x App x y\).  We start with the type A.  A lambda abstraction must be a type of the form \(B \to C\), so we unify and say \(A = B\to C\).  The variable \(x\) has to be of type \(B\), and the application \(App x y\) has to be of type \(C\).  We get a type \(C\) from an application of a term \(t\) to a term \(t'\) when \(t\) has the type \(D \to C\) and \(t'\) has the type \(D\), so we know that \(y\) has type \(D\) and \(x\) has type \(D \to C\).  Unifying with the previous type of \(x\), we find that \(B = D \to C\).  So \(x\) has type \(D \to C\), \(y\) has type \(D\), and the resulting type for the entire term is \((D \to C) \to C\).

Suppose we restrict to the set \(T_1\) of terms with one free variable.  Then given the function \(free:T_1 \to V\) that picks out the free variable and a set \(Y\) of types, then we get a type judgement function \(j:(V \to Y) \to (T_1 \to Y)\) that maps a typing context \(\Gamma: V\to Y\) and a term to the type of the term.  This typing judgement function together with composition defined in terms of application and beta lets us consider terms to be morphisms between types.  Lambek showed that well-typed terms with one free variable modulo the relations alpha, beta, and eta form the free cartesian closed category on the types \(Y\).

Lambda calculus and Turing machines both describe the computation performed by a single machine.  Nowadays, the majority of computation is done over a network.  Information can arrive form multiple sources at random times, and the order in which messages arrive matters: consider two people trying to buy the last ticket to a concert.  To model this kind of computational setup, the reduction relation obviously can't be confluent any more.

In the late 70s and early 80s, Milner and Hoare developed the notion of a "prcess calculus" that explicitly talked about passing messages over "channels", like radio or TV channels.  The terms in the calculus described how to react to various messages.  Both notions of process calculus had a fixed topology: each node in a network had a fixed set of incoming and outgoing channels.  In 1991, Milner published a new calculus called the pi calculus in order to allow for networks with a dynamic topology and to allow for messages to be sent asynchronously, racing to their intended recipient.  In 1992, Milner showed that one can embed lambda calculus into pi calculus, so it's a model for universal computation.


