Greg Meredith and I have a short paper that's been accepted to HDRA 2015 on modeling the asynchronous polyadic pi calculus with 2-categories.  We avoid domain theory entirely and model the operational semantics directly; full abstraction is almost trivial.  As a nice side-effect, we get a new tool for reasoning about consumption of resources during a computation.  

It's a small piece of a much larger project, which I'd like to describe here in a series of posts.

<h3>History</h3>

Alonzo Church invented the lambda calculus as part of his attack on Hilbert's third problem, also known as the <i>Entscheidungsproblem</i>, which asked for an algorithm to solve any mathematical problem.  Church published his proof that no such algorithm exists in 1936.  Turing invented his eponymous machines, also to solve the <i>Entscheidungsproblem</i>, and published his indepentent proof a few months after Church.  When he discovered that Church had beaten him to it, Turing proved in 1937 that the two approches were equivalent in power.  Since Turing machines were much more "mechanical" than the lambda calculus, the development of computing machines relied far more on Turing's approach, and it was only decades later that people started writing compilers for more friendly programming languages.  I've heard it quipped that "the history of programming languages is the piecemeal rediscovery of the lambda calculus by computer scientists."

The lambda calculus consists of a set of "terms" together with some relations on the terms that tell how to "run the program".  Terms are built up out of "term constructors"; in the lambda calculus there are three: \(Var\) for variables, \(Lam\) for defining functions (Church denoted this operation with the Greek letter lambda, hence the name of the calculus), and \(App\) for applying those functions to inputs.  I'll talk about these constructors and the relations more below.

Church introduced the notion of "types" to avoid programs that never stop.  Modern programming languages also use types to avoid programmer mistakes and encode properties about the program, like proving that secret data is inaccessible outside certain parts of the program.  The "simply-typed" lambda calculus starts with a set of base types and takes the closure under the binary operation \(\to\) to get a set of types \(Y\).  Each term is assigned a type; from this one can deduce the types of the variables used in the term.  An assignment of types to variables is called a <i>typing context</i>.

The search for a semantics for variants of the lambda calculus has typically been concerned with finding sets or "domains" such that the interpretation of each lambda term is a function between domains.  <a href="http://en.wikipedia.org/wiki/Scott_domain>Scott</a> worked out a domain \(D\) such that the <a href="http://math.andrej.com/2006/03/27/sometimes-all-functions-are-continuous/">continuous functions</a> from \(D\) to itself are precisely the computable ones</a>.  <a href="http://www.cambridge.org/us/academic/subjects/mathematics/logic-categories-and-sets/introduction-higher-order-categorical-logic">Lambek and Scott</a> generalized the category where we look for semantics from Set to arbitrary cartesian closed categories (CCCs).

Lambek and Scott needed to construct a CCC out of lambda terms; we call this category the <i>syntactical category</i>.  Then a structure-preserving functor from the syntactical category to Set or some other CCC would provide the semantics.  The syntactical category has types as objects and equivalence classes of certain terms as morphisms.

John Baez has a set of <a href="http://math.ucr.edu/home/baez/QG.html">lecture notes</a> from <a href="http://math.ucr.edu/home/baez/qg-fall2006/">Fall 2006</a> <a href="http://math.ucr.edu/home/baez/qg-winter2007/">through</a> <a href="http://math.ucr.edu/home/baez/qg-spring2007/">Spring 2007</a> describing Lambek's approach to the category theory of lambda calculus and generalizing it from cartesian closed categories to symmetric monoidal closed categories so it can apply to quantum computation as well: rather than taking a functor from the syntactical category into Set, we can take a functor into Hilb instead.  He and I also have a <a href="http://arxiv.org/abs/0903.0340">"Rosetta stone" paper</a> summarizing the ideas.

Note that their approach does not have the sets of terms or variables as objects!  The algebra that defines the set of terms plays only a minor role in the category; there's no morphism in the CCC, for instance, that takes a term \(t\) and a variable \(x\) to produce the term \(Lam x t\).  This failure to capture the structure of the term in the morphism wasn't a big deal for lambda calculus because of "confluence" (see below), but it turns out to matter a lot more in calculi like <a href="http://www.lfcs.inf.ed.ac.uk/reports/91/ECS-LFCS-91-180/">Milner's pi calculus</a> that describe communicating over a network, where messages can be delayed and arrival times matter for the end result (consider, for instance, two people trying to buy online the last ticket to a concert).

The last few decades have seen domains becoming more and more complicated in order to try to "unerase" the information about the structure of terms that gets lost in this approach and recover the operational semantics.  <a href="http://www.sciencedirect.com/science/article/pii/S0890540102929688">Fiore, Moggi, and Sangiorgi</a>, <a href="http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.62.4498&rep=rep1&type=pdf">Stark</a> and <a href="http://homepages.inf.ed.ac.uk/stark/prempc.html">Cattani, Stark, and Winskel</i> all present domain models of the pi calculus that involve the powerset in order to talk about all the possible futures for a term.  In the meantime, the industry has always been about operational semantics: the Java Virtual Machine is an operational semantics for the Java language.

<h3>What we did</h3>

Greg Meredith and I set out to model the operational semantics of the pi calculus directly in a higher category rather than using domain theory.  An obvious first question is, "what about types?"  I was particularly worried about how to relate this approach to the kind of thing Scott and Lambek did.  Though it didn't make it into the HDRA paper and the details won't make it into this post, we found that we're able to use the "<a href="http://noamz.org/papers/funts.pdf">type-refinement-as-a-functor</a>" idea of Mellies and Zeilberger to show how the algebraic term-constructor functions relate to the morphisms in the syntactical category.

We're hoping that this categorical approach to modeling process calculi will help with reasoning about practical situations where we want to compose calculi; for instance, we'd like to put a hundred pi calculus engines around the edges of a chip and some <a href="http://en.wikipedia.org/wiki/Ambient_calculus">ambient calculus</a> engines, which have nice features for managing the location of data, in the middle to distribute work among them.

<h3>Lambda calculus</h3>

The lambda calculus consists of a set of "terms" together with some relations on the terms.  The set \(T\) of terms is defined recursively, parametric in a countably infinite set \(V\) of variables.  The base terms are the variables: if \(x\) is an element of \(V\), then \(Var x\) is a term in \(T\).  Next, given any two terms \(t, t' \in T\), we can apply one to the other to get \(App\, t\, t'\).  Finally, we can abstract out a variable from a term: given a variable \(x\) and a term \(t,\) we get a term \(Lam\, x\, t\).

The term constructors define an algebra, a functor \(LC\) from Set to Set that takes any set of variables \(V\) to the set of terms \(T = LC(V)\).  The term constructors themselves become functions:
\[
\begin{array}{rl}
Var\colon & V \to T\\
App\colon & T \times T \to T\\
Lam\colon & V \times T \to T
\end{array}
\]

Church described three relations on terms.  The first relation, alpha, relates any two lambda abstractions that differ only in the variable name.  This is exactly the same as when we consider the function \(f(x) = x^2\) to be identical to the function \(f(y) = y^2\).  The third relation, eta, says that there's no difference between a function \(f\) and a "middle-man" function that gets an input \(x\) and applies the function \(f\) to it: \(Lam\, x\, App\, f\, x = f\).  Both alpha and eta are equivalences.

The really important relation is the second one, "beta reduction".  In order to define beta reduction, we have to define the <i>free</i> variables of a term: a variable occurring by itself is free; the set of free variables in an application is the union of the free variables in its subterms; and the free variables in a lambda abstraction are the free variables of the subterm except for the abstracted variable.
\[
\begin{array}{rl}
FV(Var\, x) = & \{x\} \\
FV(App\, t\, t') = & FV(t) \cup FV(t') \\
FV(Lam\, x\, t) = & FV(t) / \{x\} \\
\end{array}
\]

<i>Beta reduction</i> says that when we have a lambda abstraction \(Lam\, x, t\) applied to a term \(t'\), then we replace every free occurrence of \(x\) in \(t\) by \(t'\):
\[ App\, Lam\, x\, t\, t' \downarrow_\beta t{t' / x},\]
where we read the right hand side as "\(t\) with \(t'\) replacing \(x\)".  We see a similar replacement of \(y\) in action when we compose the following functions:
\[
\begin{array}{rl}
f(x) = & x + 1 \\
g(y) = & y^2 \\
g(f(x)) = & (x + 1)^2 \\
\end{array}
\]

We say a term has a normal form if there's some sequence of beta reductions that leads to a term where no beta reduction is possible.  When the beta rule applies in more than one place in a term, it doesn't matter which one you choose to do first: any sequence of betas that leads to a normal form will lead to the same normal form.  This property of beta reduction is called <i>confluence</i>.  Confluence means that the order of performing various subcomputations doesn't matter so long as they all finish: in the expression \((2 + 5) * (3 + 6)\) it doesn't matter which addition you do first or whether you distribute the expressions over each other; the answer is the same.

"Running" a program in the lambda calculus is the process of computing the normal form by repeated application of beta reduction, and the normal form itself is the result of the computation.  Confluence, however, does not mean that when there is more than one place we could apply beta reduction, we can choose any beta reduction and be guaranteed to reach a normal form.  The following lambda term, customarily denoted \(\omega\), takes an input and applies it to itself:
\[\omega = Lam\, x\, App\, x\, x\]
If we apply \(\omega\) to itself, then beta reduction produces the same term, customarily called \(Omega\):
\[\Omega = App\, \omega\, \omega\]
\[\Omega \downarrow_\beta \Omega.]
It's an infinite loop!  Now consider this lambda term that has \(\Omega\) as a subterm:
\[App\, (App\, (Lam\, x\, Lam\, y\, x)\, (Lam\, x\, x))\, \Omega.]
It says, "Return the first element of the pair (identity function, \(Omega\))".  If it has an answer at all, the answer should be "the identity function".  The question becomes, "Do we try to calculate the elements of the pair before applying the projection to it?"

<h3>Lazy lambda calculus</h3>

Many programming languages, like Java, C, JavaScript, Perl, Python, and Lisp are "eager": they calculate the normal form of inputs to a function before calculating the result of the function on the inputs; the expression above, implemented in any of these languages, would be an infinite loop.  Other languages, like Miranda, Lispkit, Lazy ML, Haskell and its predecessor Orwell are "lazy" and only apply beta reduction to inputs when they are needed to complete the computation; in these languages, the result is the identity function.  Abramsky wrote a <a href="https://www.cs.ox.ac.uk/files/293/lazy.pdf">48-page paper</a> about constructing a domain that captures the operational semantics of lazy lambda calculus.

The idea of representing operational semantics directly with higher categories originated with <a href="http://www.math.mcgill.ca/rags/WkAdj/LICS.pdf">R. A. G. Seely</a>, who suggested that beta reduction should be a 2-morphism; <a href="http://math.ucr.edu/home/baez/qg-winter2007/hilken_2lambda_calculus.ps">Barney Hilken</a> and <a href="https://hal.archives-ouvertes.fr/hal-00540205/file/macy.pdf">Tom Hirschowitz</a> have also contributed to looking at lambda calculus from this perspective.  In the "Rosetta stone" paper that John Baez and I wrote, we made an analogy between programs and Feynman diagrams.  The analogy is precise as far as it goes, but it's unsatisfactory in the sense that Feynman diagrams describe processes happening over time, while Lambek and Scott mod out by the process of computation that occurs over time.  If we use 2-categories that explicitly model rewrites between terms, we get something that could potentially be interpreted in 2-Hilb: types would become analogous to particles, terms would become analogous to space, and rewrites would happen over time.
 
The idea from the "algebra of terms" perspective is that we have objects \(V\) and \(T\) for variables and terms, term constructors as 1-morphisms, and the nontrivial 2-morphisms generated by beta reduction.  Seely showed that this approach works fine when you're unconcerned with the context in which reduction can occur.  

This approach, however, doesn't work for lazy lambda calculus!  Horizontal composition in a 2-category is a functor, so if a term \(t\) reduces to a term \(t'\), then by functoriality, \(Lam\, x\, t\) must reduce to \(Lam\, x\, t'\)---but this is forbidden in the lazy lambda calculus!  Functoriality of horizontal composition is a "relativity principle" in the sense that reductions in one context are the same as reductions in any other context.  In lazy programming languages, on the other hand, the "head" context is privileged: reductions only happen here.  It's somewhat like believing that measuring differences in temperature is like measuring differences in space, that only the difference is meaningful---and then discovering absolute zero.  When beta reduction can happen anywhere in a term, there are <i>too many 2-morphisms</i> to model lazy lambda calculus.

In order to model this special context, we reify it: we add a special unary term constructor \(Ctx\colon T \to T\) that marks contexts where reduction is allowed, then redefine beta reduction so that wherever we had \(t \downarrow_\beta t'\) before, we now require \(Ctx\, t \downarrow_\beta Ctx\, t'\).  \(Ctx\) behaves like a catalyst that enables the beta reduction to occur.  This lets us cut down the set of 2-morphisms to exactly those that are allowed in the lazy lambda calculus; Greg and I did essentially the same thing in the pi calculus.

<h3>Compute resources</h3>

In order to run a program that does anything practical, you need a processor, time, memory, and perhaps disk space or a network connection or a display.  All of these resources have a cost, and it would be nice to keep track of them.  One side-effect of reifying the context is that we can use it as a resource.  

Say we have two terms that we'd like to run in parallel; by using two copies of the resource \(Ctx\), we're counting processors.  

Suppose that instead of having the beta rule \(Ctx\, t \downarrow_\beta Ctx\, t'\), we have \(Ctx\, t \downarrow_\beta t'\): beta consumes the resource \(Ctx\).  By supplying a term with a fixed number of uses of \(Ctx\), we can limit the number of computation steps that can occur.  

If instead of a unary term constructor \(Ctx\colon T \to T\) we have a nullary constructor \(c\colon 1 \to T\), then we can both recover the original unary term constructor by \(Ctx\, t = App\, c\, t\) and have \(c\) be a value that can be passed around to control what part of the program gets the compute resource.

These are just the first things that come to mind; we're experimenting with variations.

<h3>Conclusion</h3>

We figured out how to model the operational semantics of a term calculus directly in a 2-category by requiring a catalyst to carry out a rewrite, which gave us full abstraction without needing a domain based on representing all the possible futures of a term.  As a side-effect, it also gave us a new tool for modeling resource consumption in the process of computation.  Though I haven't explained how yet, there's a nice connection between the "algebra-of-terms" approach that uses \(V\) and \(T\) as objects and Lambek and Scott's approach that uses types as objects that uses Mellies and Zeilberger's ideas about type refinement.  Next time, I'll talk about the pi calculus and types.

----------

[[Denotational vs operational semantics, full abstraction]]


Lambda calculus has the property, called <i>confluence</i>, that any two sequences of rewrites of an initial term that reach a final term will reach the same final term.    

Modern computation, however, happens over networks, where the relative order of messages arriving matters a great deal: consider two people trying to buy the last ticket to a concert.  Process calculi were developed to model systems where the outcome depends on a race to acquire a resource.  

John Baez has a set of <a href="http://math.ucr.edu/home/baez/QG.html">lecture notes</a> from <a href="http://math.ucr.edu/home/baez/qg-fall2006/">Fall 2006</a> <a href="http://math.ucr.edu/home/baez/qg-winter2007/">through</a> <a href="http://math.ucr.edu/home/baez/qg-spring2007/">Spring 2007</a> describing Lambek's approach to the category theory of lambda calculus and generalizing it from cartesian closed categories to symmetric monoidal closed categories so it can apply to quantum computation as well.  He and I also have a <a href="http://arxiv.org/abs/0903.0340">paper</a> summarizing the ideas.  In our paper, we show how alpha-beta-eta-equivalence classes of programs are analogous to the quantum processes typically described by string diagrams like Feynman diagrams.

One problem with this approach is a mismatch around <i>time</i>: the execution of a computer program happens over time, but in Lambek's approach, we mod out by the very relation (called "beta reduction) that describes a processing step.  Rewrites are morally 2-morphisms.  Various people like <a href="http://www.math.mcgill.ca/rags/WkAdj/LICS.pdf">R. A. G. Seely</a>, <a href="http://math.ucr.edu/home/baez/qg-winter2007/hilken_2lambda_calculus.ps">Barney Hilken</a>, and <a href="https://hal.archives-ouvertes.fr/hal-00540205/file/macy.pdf">Tom Hirschowitz</a> have all contributed to looking at lambda calculus from this perspective.  

Lambda calculus and Turing machines both describe the computation performed by a single machine.  Nowadays, the majority of computation is done over a network.  Information can arrive form multiple sources at random times, and the order in which messages arrive matters: consider two people trying to buy the last ticket to a concert.  To model this kind of computational setup, the reduction relation obviously can't be confluent any more.

In the late 70s and early 80s, Milner and Hoare developed the notion of a "process calculus" that explicitly talked about passing messages over "channels", like radio or TV channels.  The terms in the calculus described how to react to various messages.  Both notions of process calculus had a fixed topology: each node in a network had a fixed set of incoming and outgoing channels.  In 1989, Milner published a new calculus called the pi calculus in order to allow networks with a dynamic topology and to allow messages to be sent asynchronously, racing to their intended recipient.  In 1990, Milner showed that one can embed lambda calculus into pi calculus, so it's a model for universal computation.

Like lambda calculus, pi calculus is parametric in a set of channels \(V\) (also known as names or variables).  Pi calculus has the following term constructors:
\[
\begin{array}{rll}
0\colon & 1 \to T & \mbox{do nothing} \\
|\colon & (T \times T) \to T & \mbox{run concurrently} \\
\forall n \in \mathbb{N},\;?_n\colon & (V \times (V^n \to T)) \to T & \mbox{wait for input} \\
\forall n \in \mathbb{N},\;!_n\colon & (V \times V^n) \to T & \mbox{output} \\
\nu\colon & (V \times T) \to T & \mbox{new channel}\\
\rho\colon & T \to T & \mbox{replication}
\end{array}
\]



In our paper, we show how certain equivalence classes of programs are analogous to the quantum processes typically described by string diagrams like Feynman diagrams.



The Hindley-Milner typing algorithm uses a coalgebra instead of an algebra.  If we start with a term of type \(A\), we can either generate a variable of type \(A\), an application of a term of type \(B \multimap A\) to a term of type \(B\), or the abstraction of a variable of type \(B\) from a term of type \(A\).  The term destructors become functions between sets annotated by types:
\[
\begin{array}{rl}
Var\colon & (T\times A) \to (V\times A)\\
App_B\colon & (T\times A) \to (T\times (B \to A)) \times (T\times B)\\
Lam_B\colon & (T\times A) \to (V\times B) \times (T\times A)
\end{array}
\]

Suppose we want the most general type for the term \(Lam\, x\, App\, x\, y\).  We start with the type \(A\).  A lambda abstraction must be a type of the form \(B \to C\), so we unify and say \(A = B\to C\).  The variable \(x\), therefore, has to be of type \(B\), and the application \(App\, x\, y\) has to be of type \(C\).  The only way we get a type \(C\) from an application of a term \(t\) to a term \(t'\) is when \(t\) has the type \(D \to C\) and \(t'\) has the type \(D\); in this case, it means that \(x\) has type \(D \to C\) and \(y\) has type \(D\).  Unifying with the previous type of \(x\), we find that \(B = D \to C\).  So \(x\) has type \(D \to C\), \(y\) has type \(D\), and the resulting type for the entire term is \((D \to C) \to C\).

When we take the 

This assignment of types to the variables \(x\) and \(y\) is called a "typing context."  Lambek and Scott showed how to treat the types in the typing context as the source of a morphism and the type of the term as the target of the morphism.  Since there's no ordering on the variables, they restricted to the set \(T_1\) of terms with one free variable.  Then given the function \(free:T_1 \to V\) that picks out the free variable and a set \(Y\) of types, we get a type judgement function \(j:(V \to Y) \to (T_1 \to Y)\) that maps a typing context \(\Gamma: V\to Y\) and a term to the type of the term.  This typing judgement function together with composition defined in terms of application and beta lets us consider terms to be morphisms between types.  Lambek and Scott showed that well-typed terms with one free variable modulo the relations alpha, beta, and eta form the free cartesian closed category on the types \(Y\).




